---
layout: page
title: 2.3 Least squares and Nearest Neighbors
<!-- permalink: /esl/chapter_2_3/ -->
usemathjax: true
---

# Least squares linear regression

Linear regression model assumes $$\hat{Y} = X^T \hat{\beta}$$.

How do we decide the best $$\hat{\beta}$$? Least squares (minimizing residual sum of squares) is the most popular criteria.
- TODO: "RSS is quadratic function of $$\hat{\beta}$$. So minimum always exists, but may not be unique."

If $$X^T X$$ is nonsingular, unique solution is $$\hat{\beta}=(X^T X)^{-1}X^T y$$. 
- TODO: derivation.
- TODO: geometry and "entire fitted surface is characterized by p parameters. Intuitively, we do not need a very large data to fit such a model."

We can use linear model in 2-class classification context.
- Encode blue as 0 and orange as 1. If $$\hat{Y} \geq 0.5$$, predict orange.
- Decision boundary (subset of $$X$$ where $$\hat{Y} = 0.5$$) is linear (forms a hyperplane).

# KNN

KNN predicts $$\hat{Y} = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i,$$ the proportion of orange in $$N_k$$.
- For $$N_k,$$ we can use Euclidean distance for closeness measure.

How do we decide the best $$k$$?
- Training error is appx increasing function of $$k$$ and always 0 for $$k = 1$$. TODO: why?
- Can't use sum of squares, because always choose $$k=1$$.
- Effective # of params is $$N/k,$$ generally bigger than $$p$$. TODO: intuition makes sense. Why, more formally?

# Least squares and KNN

Linear decision boundary is smooth and more stable to fit, but relies heavily on model assumption. Low variance and high bias.

TODO: relation to data generation

TODO: extensions.
















