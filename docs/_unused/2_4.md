---
layout: page
title: 2.4 Statistical Decision Theory
<!-- permalink: /esl/chapter_2_4/ -->
usemathjax: true
---

Suppose a regression problem. If squared error loss is used to measure best prediction, best prediction of $$Y$$ at any point $$x$$ is the conditional expectation, $$E[Y \lvert X = x]$$.

KNN attempts to implement this best prediction with two approximations:
1. expectation is approximated by averaging over training data.
1. conditioning at $$X = x$$ is relaxed to its neighborhood.

TODO(p19): convergence.

However,
1. we often do not have very large training data.
1. in higher dimensions, neighborhood is large.

If more structured model is appropriate, we can get a more stable estimate.

TODO(p19): "stable" definition.

Linear regression with least squares approximates expectation with average.
